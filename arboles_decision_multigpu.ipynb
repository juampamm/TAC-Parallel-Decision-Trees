{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "prueba_hilos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "71ec9fe4"
      },
      "source": [
        "# CART on the Bank Note dataset\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "import multiprocessing\n",
        "import time\n",
        "import random\n",
        "\n",
        "PARALLEL = True\n",
        "PARALLEL2 = False\n",
        "\n",
        "THREADS = 4\n",
        "NUM_ATRIBUTES = 20\n",
        "SAMPLES = 1000\n",
        "N_FOLDS = 5\n",
        "\n",
        "\n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "    file = open(filename, \"rt\")\n",
        "    lines = reader(file, delimiter=';')\n",
        "    dataset = list(lines)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "    for row in dataset:\n",
        "        row[column] = float(row[column].strip())\n",
        "\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = list(dataset)\n",
        "    fold_size = int(len(dataset) / n_folds)\n",
        "    for i in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "\n",
        "def eval_parallel(valores):\n",
        "    train_set = list(valores[0])\n",
        "    train_set.remove(valores[1])\n",
        "    train_set = sum(train_set, [])\n",
        "    test_set = list()\n",
        "    for row in valores[1]:\n",
        "        row_copy = list(row)\n",
        "        test_set.append(row_copy)\n",
        "        row_copy[-1] = None\n",
        "    predicted = valores[2](train_set, test_set, valores[3][0], valores[3][1])\n",
        "    actual = [row[-1] for row in valores[1]]\n",
        "    accuracy = accuracy_metric(actual, predicted)\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "    scores = list()\n",
        "    if PARALLEL2 == True:\n",
        "        valores = []\n",
        "        for ii in range(n_folds):\n",
        "            aux = []\n",
        "            aux.append(folds)\n",
        "            aux.append(folds[ii])\n",
        "            aux.append(algorithm)\n",
        "            aux.append(args)\n",
        "            valores.append(aux)\n",
        "        #valores = [[folds, folds[0], algorithm, args], [folds, folds[1], algorithm, args], [folds, folds[2], algorithm, args], [folds, folds[3], algorithm, args], [folds, folds[4], algorithm, args]]\n",
        "\n",
        "        pool = multiprocessing.Pool(THREADS)   \n",
        "        scores = pool.map(eval_parallel, valores)\n",
        "        pool.close()\n",
        "    else:\n",
        "        for fold in folds:\n",
        "            train_set = list(folds)\n",
        "            train_set.remove(fold)\n",
        "            train_set = sum(train_set, [])\n",
        "            test_set = list()\n",
        "            for row in fold:\n",
        "                row_copy = list(row)\n",
        "                test_set.append(row_copy)\n",
        "                row_copy[-1] = None\n",
        "            predicted = algorithm(train_set, test_set, *args)\n",
        "            actual = [row[-1] for row in fold]\n",
        "            accuracy = accuracy_metric(actual, predicted)\n",
        "            scores.append(accuracy)\n",
        "    return scores\n",
        "\n",
        "\n",
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "    left, right = list(), list()\n",
        "    for row in dataset:\n",
        "        if row[index] < value:\n",
        "            left.append(row)\n",
        "        else:\n",
        "            right.append(row)\n",
        "    return left, right\n",
        "\n",
        "\n",
        "# def prueba(valores):\n",
        "\n",
        "\n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "    # count all samples at split point\n",
        "    n_instances = float(sum([len(group) for group in groups]))\n",
        "    # sum weighted Gini index for each group\n",
        "    gini = 0.0\n",
        "    for group in groups:\n",
        "        size = float(len(group))\n",
        "        # avoid divide by zero\n",
        "        if size == 0:\n",
        "            continue\n",
        "        score = 0.0\n",
        "        # score the group based on the score for each class\n",
        "        for class_val in classes:\n",
        "            p = [row[-1] for row in group].count(class_val) / size\n",
        "            score += p * p\n",
        "        # weight the group score by its relative size\n",
        "        gini += (1.0 - score) * (size / n_instances)\n",
        "    return gini\n",
        "\n",
        "\n",
        "def evaluate_row(valores):\n",
        "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "    for row in valores[2]:\n",
        "        groups = test_split(valores[0], row[valores[0]], valores[2])\n",
        "        gini = gini_index(groups, valores[1])\n",
        "        if gini < b_score:\n",
        "            b_index, b_value, b_score, b_groups = valores[0], row[valores[0]], gini, groups\n",
        "    return [b_index, b_value, b_score, b_groups]\n",
        "\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "    if PARALLEL == True:\n",
        "        valores = []\n",
        "        for ii in range(NUM_ATRIBUTES):\n",
        "            aux = []\n",
        "            aux.append(ii)\n",
        "            aux.append(class_values)\n",
        "            aux.append(dataset)\n",
        "            valores.append(aux)\n",
        "        pool = multiprocessing.Pool(THREADS) \n",
        "        resultados = pool.map(evaluate_row, valores)\n",
        "        \n",
        "\n",
        "        pool.close()\n",
        "        for ii in range(NUM_ATRIBUTES):\n",
        "            if b_score > resultados[ii][2]:\n",
        "                b_index, b_value, b_score, b_groups = resultados[ii][0], resultados[ii][1], resultados[ii][2], resultados[ii][3]\n",
        "    else:\n",
        "        for index in range(len(dataset[0]) - 1):\n",
        "            for row in dataset:\n",
        "                groups = test_split(index, row[index], dataset)\n",
        "                gini = gini_index(groups, class_values)\n",
        "                if gini < b_score:\n",
        "                    b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\n",
        "\n",
        "    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
        "\n",
        "\n",
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "    outcomes = [row[-1] for row in group]\n",
        "    return max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "\n",
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, depth):\n",
        "    left, right = node['groups']\n",
        "    del (node['groups'])\n",
        "    # check for a no split\n",
        "    if not left or not right:\n",
        "        node['left'] = node['right'] = to_terminal(left + right)\n",
        "        return\n",
        "    # check for max depth\n",
        "    if depth >= max_depth:\n",
        "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "        return\n",
        "    # process left child\n",
        "    if len(left) <= min_size:\n",
        "        node['left'] = to_terminal(left)\n",
        "    else:\n",
        "        node['left'] = get_split(left)\n",
        "        split(node['left'], max_depth, min_size, depth + 1)\n",
        "    # process right child\n",
        "    if len(right) <= min_size:\n",
        "        node['right'] = to_terminal(right)\n",
        "    else:\n",
        "        node['right'] = get_split(right)\n",
        "        split(node['right'], max_depth, min_size, depth + 1)\n",
        "\n",
        "\n",
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size):\n",
        "    root = get_split(train)\n",
        "    split(root, max_depth, min_size, 1)\n",
        "    return root\n",
        "\n",
        "\n",
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "    if row[node['index']] < node['value']:\n",
        "        if isinstance(node['left'], dict):\n",
        "            return predict(node['left'], row)\n",
        "        else:\n",
        "            return node['left']\n",
        "    else:\n",
        "        if isinstance(node['right'], dict):\n",
        "            return predict(node['right'], row)\n",
        "        else:\n",
        "            return node['right']\n",
        "\n",
        "\n",
        "# Classification and Regression Tree Algorithm\n",
        "def decision_tree(train, test, max_depth, min_size):\n",
        "    tree = build_tree(train, max_depth, min_size)\n",
        "    #print_tree(tree)\n",
        "    predictions = list()\n",
        "    for row in test:\n",
        "        prediction = predict(tree, row)\n",
        "        predictions.append(prediction)\n",
        "    return (predictions)\n",
        "\n",
        "# Print a decision tree\n",
        "def print_tree(node, depth=0):\n",
        "    if isinstance(node, dict):\n",
        "        print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
        "        print_tree(node['left'], depth+1)\n",
        "        print_tree(node['right'], depth+1)\n",
        "    else:\n",
        "        print('%s[%s]' % ((depth*' ', node)))\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Test CART on Bank Note dataset\n",
        "    seed(1)\n",
        "    # load and prepare data\n",
        "    # filename = 'prueba_csv.csv'\n",
        "    # dataset = load_csv(filename\n",
        "\n",
        "    dataset = []\n",
        "    for ii in range(SAMPLES):\n",
        "        atributes = []\n",
        "        for jj in range(NUM_ATRIBUTES):\n",
        "            atributes.append(random.randint(0, 100000))\n",
        "        atributes.append(random.randint(0, 1))\n",
        "        dataset.append(atributes)\n",
        "    # convert string attributes to integers\n",
        "    # for i in range(len(dataset[0])):\n",
        "    #   str_column_to_float(dataset, i)\n",
        "    # evaluate algorithm\n",
        "    inicio = time.time()\n",
        "    max_depth = 5\n",
        "    min_size = 10\n",
        "    scores = evaluate_algorithm(dataset, decision_tree, N_FOLDS, max_depth, min_size)\n",
        "    print('Scores: %s' % scores)\n",
        "    print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))\n",
        "\n",
        "    fin = time.time()\n",
        "    print('Time: ', fin - inicio)\n"
      ],
      "id": "71ec9fe4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c33777e2"
      },
      "source": [
        "THREADS = 1\n",
        "main()"
      ],
      "id": "c33777e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63bc932a"
      },
      "source": [
        "THREADS = 4\n",
        "main()"
      ],
      "id": "63bc932a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a2c9820"
      },
      "source": [
        "THREADS = 8\n",
        "main()"
      ],
      "id": "7a2c9820",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "907fabb5"
      },
      "source": [
        "THREADS = 12\n",
        "main()"
      ],
      "id": "907fabb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e2b12d5"
      },
      "source": [
        "THREADS = 16\n",
        "main()"
      ],
      "id": "1e2b12d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc782312"
      },
      "source": [
        "THREADS = 20\n",
        "main()"
      ],
      "id": "bc782312",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bce7d28"
      },
      "source": [
        "THREADS = 24\n",
        "main()"
      ],
      "id": "2bce7d28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e0767ef"
      },
      "source": [
        "THREADS = 28\n",
        "main()"
      ],
      "id": "8e0767ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "931aa02c"
      },
      "source": [
        "THREADS = 32\n",
        "main()"
      ],
      "id": "931aa02c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "955ada23"
      },
      "source": [
        "THREADS = 36\n",
        "main()"
      ],
      "id": "955ada23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ec2a359"
      },
      "source": [
        "THREADS = 40\n",
        "main()"
      ],
      "id": "6ec2a359",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxUHBX7uvNhr"
      },
      "source": [
        "THREADS = 44\n",
        "main()"
      ],
      "id": "SxUHBX7uvNhr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR11lGjuvN9a"
      },
      "source": [
        "THREADS = 48\n",
        "main()"
      ],
      "id": "pR11lGjuvN9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5v3D9n4vOP4"
      },
      "source": [
        "THREADS = 52\n",
        "main()"
      ],
      "id": "c5v3D9n4vOP4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFnj-OH-vSiL"
      },
      "source": [
        "THREADS = 56\n",
        "main()"
      ],
      "id": "oFnj-OH-vSiL",
      "execution_count": null,
      "outputs": []
    }
  ]
}